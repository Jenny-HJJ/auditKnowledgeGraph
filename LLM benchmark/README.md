# 审计大语言模型评估任务

### 审计命名实体识别（NER）

第一个数据集包含三种典型的实体类型，即人名（PER）、机构名（ORG）和法律法规名（auditLaw）；第二种数据集包含了 10 种类型的实体名称，即审计基本要素，并将
所有的审计类型合并为一个大类；第三种为细粒度审计命名实体，即包含了所有不同类型的审计命名实体。

### 关系分类（RC, relation classification）

该任务基于从句子中提取出的两个审计实体及该句子文本，根据关系类型选项预测该实体对之间的关系。

### 多项选择(Multiple Choice) 

根据关于审计知识的问题描述，从四个选项中选择最佳答案。该任务从审计师考试题库中构建了两种数据集，即（1）审计概念相关选择和（2）会计数值计算选择。
文本分类(Text classification) 基于从文档层抽取的审计三元组 (e,r,d) ，本文开发了三种文本分类任务，即预测文档d所属的审计类型e。

### 自动问答(Question Answering)

 即基于提供的问题描述自动回答审计问题的任务。该任务包含两种自动问答数据集：（1）基于审计知识图谱构建审计常识知识，这里的答案是简短的。（2）基于包含长文档实体的审计知识图谱构建审计概念问答数据集，这里需要回答审计概念的定义描述。

### 自动摘要(Summarization) 

根据审计相关描述文本用一个短语或简短的句子总结该文本。该任务包含两种类型：（1）基于审计案例描述文本用一句话总结该审计案例的主要内容；（2）基于审计疑点描述信息用一个短语总结该审计疑点。



## 数据集汇总

| 任务名称            | 数据集名称                | 指令数据数量 |
| ------------------- | ------------------------- | ------------ |
| NER                 | Audit_NER_3               | 137          |
|                     | Audit_NER_10              | 284          |
|                     |                           |              |
| RC                  | AuditRC                   | 216          |
| Multiple Choice     | ConceptChoice             | 202          |
|                     | Numerical calculation     | 103          |
| Text classification | CaseClassification        | 317          |
|                     | FraudClassification       | 884          |
|                     | LawClassification         | 282          |
|                     | AuditEntityClassification | 1578         |
| Question Answering  | ShortQA                   | 334          |
|                     | DefinitionQA              | 333          |
| Summarization       | AuditFraudSummary         | 326          |
|                     | CaseTitleSummary          | 149          |



## 实验结果

| **Benchmark**             | **Metrics**   | **LLaMA-2-7B**    | **Chinese-LLaMA-2-7B** | **ChatGLM3-6B**   | **Baichuan-7B** | **Qwen-7B**      | **GPT-3.5**       | **GPT-4**         |
| ------------------------- | ------------- | ----------------- | ---------------------- | ----------------- | --------------- | ---------------- | ----------------- | ----------------- |
| Audit_NER_3(5-fewshot)    | Entity_F1     | 0.1095            | 0.1095                 | 0.1711            | 0.1237          | ***\*0.1743\**** | 0.09              | 0.122             |
| Audit_NER_10(5-fewshot)   | Entity_F1     | 0                 | 0                      | 0.0602            | 0.0401          | ***\*0.0768\**** | 0.0426            | 0.0563            |
| Audit_NER_20(5-fewshot)   | Entity_F1     | 0                 | 0                      | 0.0092            | 0.0187          | 0.017            | 0.0084            | ***\*0.0224\****  |
| auditRE                   | precision     | 0.5144            | 0.5144                 | 0.2428            | 0.3918          | 0.3173           | 0.5817            | ***\*0.7019\****  |
|                           | recall        | 0.0048            | 0.0048                 | 0.1755            | 0.2764          | ***\*0.3558\**** | 0.0024            | 0.0024            |
|                           | F1            | 0.3620            | 0.362                  | 0.2845            | 0.3321          | 0.2971           | 0.5325            | ***\*0.6871\****  |
| conceptChoice             | Acc           | 0.3377            | 0.3377                 | 0.3132            | 0.2241          | 0.0982           | 0.5783            | ***\*0.7123\****  |
| numerical calculation     | Acc           | 0.2278            | 0.235                  | ***\*0.2846\****  | 0.1074          | 0.1513           | 0.0369            | 0.0334            |
| auditcaseClassification   | F1            | 0.4853            | 0.4853                 | 0.5568            | 0.2139          | 0.1678           | 0.8740            | ***\*0.9410\****  |
|                           | Macro_F1      | 0.4751            | 0.4751                 | 0.5504            | 0.1991          | 0.1477           | 0.8647            | ***\*0.9396\****  |
| auditfraudClassification  | F1            | 0.2128            | 0.2128                 | 0.1335            | 0.1335          | 0.1335           | ***\*0.6923\****  | 0.5177            |
|                           | Macro_F1      | 0.3383            | 0.3384                 | 0.0891            | 0.0891          | 0.1015           | ***\*0.6535\****  | 0.6025            |
| auditlawClassification    | F1            | 0.4262            | 0.4262                 | 0.5392            | 0.2508          | 0.4579           | 0.3725            | ***\*0.8902\****  |
|                           | Macro_F1      | 0.3736            | 0.3736                 | 0.4474            | 0.1802          | 0.3770           | 0.3954            | ***\*0.8382\****  |
| auditentityClassification | F1            | 0.6198            | 0.6197                 | 0.6797            | 0.1899          | 0.7203           | ***\*0.7944\****  | 0.7847            |
|                           | Macro_F1      | 0.3600            | 0.3595                 | 0.4054            | 0.0902          | 0.4649           | 0.5413            | ***\*0.5583\****  |
| ShortQA                   | Bert_score_F1 | 0.6428            | 0.6428                 | 0.6603            | 0.4872          | 0.2801           | 0.7232            | ***\*0.7876\****  |
|                           | Bart_score    | -7.2696           | -7.2695                | -7.3187           | -7.7567         | -8.0446          | -6.0324           | ***\*-4.0984\**** |
| DefinitionQA              | Bert_score_F1 | 0.5792            | 0.5790                 | ***\*0.5918\****  | 0.5800          | 0.5508           | 0.4588            | 0.4543            |
|                           | Bart_score    | ***\*-5.1191\**** | -5.1234                | -5.0781           | -5.1355         | -5.0638          | -5.8627           | -5.8944           |
| AuditFraudSummary         | Bert_score_F1 | 0.4902            | 0.4917                 | 0.5180            | 0.4211          | 0.3378           | 0.7599            | ***\*0.7322\****  |
|                           | Bart_score    | -5.9441           | -5.9507                | -5.1567           | -7.9820         | -5.7731          | ***\*-4.9939\**** | -5.2386           |
| CaseTitleSummary          | Bert_score_F1 | 0.3481            | 0.3481                 | ***\*0.6264\****  | 0.3628          | 0.4456           | 0.5026            | 0.5020            |
|                           | Bart_score    | -5.6214           | -5.6177                | ***\*-4.0312\**** | -5.7287         | -6.3956          | -5.9271           | -5.9167           |
| **综合水平**              |               | 0.3458            | 0.3464                 | 0.3877            | 0.2489          | 0.2792           | 0.46189           | **0.517****4**    |



# 致谢

本项目的LLM测试代码采用 **[PIXIU](https://github.com/The-FinAI/PIXIU)** 项目提供的代码进行测试。